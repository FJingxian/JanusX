# -*- coding: utf-8 -*-
"""
JanusX: FASTQ -> VCF Pipeline Command-Line Interface

This CLI prepares metadata from a FASTQ directory and runs:
  fastp -> bwa-mem -> markdup -> gVCF -> joint-calling -> SNP filtering -> TSV

Requirements
------------
- Paired-end FASTQ files named as: *.R1.fastq.gz / *.R2.fastq.gz
- Reference FASTA with a .fai index (auto-generated by this CLI)

Examples
--------
  jx fastq2vcf \\
    --reference /data/ref.fa \\
    --fastq-dir /data/fastq \\
    --workdir /data/out \\
    --backend nohup \\
    --maxtask 2
"""

import os
import re
import time
import socket
import argparse
import subprocess
import shutil
import urllib.request
from pathlib import Path
from collections import defaultdict
from typing import List

from tqdm import tqdm

import janusx.pipeline
from janusx.pipeline.fastq2vcf import fastq2vcf, indexREF
from janusx.pipeline.pipeline import wrap_cmd
from ._common.log import setup_logging

READ_RE = re.compile(r"\.(R[12])\.(fastq|fq)(\.gz)?$", re.IGNORECASE)
FASTQ_SUFFIXES = (".fastq", ".fq", ".fastq.gz", ".fq.gz")


def downloader(url: str, dst: Path) -> None:
    dst = Path(dst)
    dst.parent.mkdir(parents=True, exist_ok=True)
    tmp = dst.with_suffix(dst.suffix + ".tmp")

    with urllib.request.urlopen(url) as r, open(tmp, "wb") as f:
        total = r.headers.get("Content-Length")
        total = int(total) if total is not None else None
        with tqdm(total=total, unit="B", unit_scale=True, unit_divisor=1024) as pbar:
            while True:
                chunk = r.read(1024 * 1024)
                if not chunk:
                    break
                f.write(chunk)
                pbar.update(len(chunk))

    os.replace(tmp, dst)


def _fastq_read_info(name: str):
    m = READ_RE.search(name)
    if not m:
        return None
    read = m.group(1).upper()
    stem = name[:m.start()]
    parts = stem.split("_")
    sample = parts[0]
    lane = parts[1] if len(parts) > 1 else None
    return sample, lane, read


def two_level_cluster(fastqlist):
    level1 = defaultdict(list)
    meta = {}

    for f in fastqlist:
        f = Path(f)
        if not f.name.lower().endswith(FASTQ_SUFFIXES):
            continue
        info = _fastq_read_info(f.name)
        if not info:
            continue
        sample, lane, read = info
        level1[sample].append(f)
        meta[f] = (sample, lane, read)

    level2 = {}
    for sample, files in level1.items():
        if len(files) == 4:
            sub = defaultdict(list)
            for f in files:
                s, lane, _ = meta[f]
                sub[(s, lane)].append(f)
            level2[sample] = dict(sub)

    return dict(level1), level2


def _pair_fastqs(files, sample_hint: str) -> tuple[Path, Path]:
    r1 = None
    r2 = None
    for f in sorted(files):
        info = _fastq_read_info(f.name)
        if not info:
            continue
        read = info[2]
        if read == "R1":
            if r1 is not None:
                raise ValueError(f"Duplicate R1 for sample {sample_hint}: {f}")
            r1 = f
        elif read == "R2":
            if r2 is not None:
                raise ValueError(f"Duplicate R2 for sample {sample_hint}: {f}")
            r2 = f

    if r1 is None or r2 is None:
        raise ValueError(f"Missing R1/R2 for sample {sample_hint}: {files}")
    return r1, r2


def build_samples(level1, level2):
    samples = {}
    for sample, files in level1.items():
        if len(files) == 2:
            r1, r2 = _pair_fastqs(files, sample)
            samples[sample] = [r1, r2]
            continue

        if len(files) == 4 and sample in level2:
            for (s, lane), lane_files in level2[sample].items():
                r1, r2 = _pair_fastqs(lane_files, f"{s}_{lane}")
                key = f"{s}_{lane}" if lane else s
                samples[key] = [r1, r2]
            continue

        raise ValueError(
            f"Sample {sample} has {len(files)} FASTQ files. "
            "Expected 2 (R1/R2) or 4 (two lanes) files."
        )
    return samples


def read_chroms_from_fai(reference: Path) -> List[str]:
    fai = Path(f"{reference}.fai")
    if not fai.exists():
        raise FileNotFoundError(f"FASTA index not found: {fai}")
    chroms: List[str] = []
    with open(fai, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            chrom = line.split("\t", 1)[0].strip()
            if chrom:
                chroms.append(chrom)
    if not chroms:
        raise ValueError(f"Empty FASTA index: {fai}")
    return chroms


def main():
    t_start = time.time()

    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )

    required_group = parser.add_argument_group("Required Arguments")
    required_group.add_argument(
        "-r", "--reference", required=True, type=str,
        help="Reference genome FASTA file.",
    )
    required_group.add_argument(
        "-i", "--fastq-dir", required=True, type=str,
        help="Directory containing FASTQ files.",
    )
    required_group.add_argument(
        "-w", "--workdir", required=True, type=str,
        help="Working directory for outputs.",
    )
    optional_group = parser.add_argument_group("Optional Arguments")
    optional_group.add_argument(
        "-b", "--backend", required=False, type=str,default='nohup',
        choices=["nohup", "csub"],
        help=f"Execution backend for the pipeline. (default: nohup)",
    )
    optional_group.add_argument(
        "-m", "--maxtask", required=False, type=int,default=1,
        help="Max concurrent jobs when backend=nohup. (default: 1)",
    )

    args = parser.parse_args()

    workdir = Path(args.workdir).expanduser().resolve()
    workdir.mkdir(parents=True, exist_ok=True)
    (workdir / "log").mkdir(parents=True, exist_ok=True)
    (workdir / "tmp").mkdir(parents=True, exist_ok=True)
    log_path = workdir / "fastq2vcf.log"
    logger = setup_logging(str(log_path))

    reference = Path(args.reference).expanduser().resolve()
    if not reference.exists():
        raise FileNotFoundError(f"Reference file does not exist: {reference}")

    fastq_dir = Path(args.fastq_dir).expanduser().resolve()
    if not fastq_dir.exists():
        raise FileNotFoundError(f"FASTQ directory does not exist: {fastq_dir}")

    logger.info("JanusX: FASTQ -> VCF pipeline")
    logger.info(f"Host: {socket.gethostname()}")
    logger.info(f"Reference: {reference}")
    logger.info(f"FASTQ dir: {fastq_dir}")
    logger.info(f"Workdir: {workdir}")
    logger.info(f"Backend: {args.backend}")
    logger.info(f"Max tasks (nohup): {args.maxtask}")

    binary_path = Path(janusx.pipeline.__file__).resolve().parent / "bin"
    binary_path.mkdir(parents=True, exist_ok=True)
    sif_path = binary_path / "janusxext.sif"
    if not sif_path.exists():
        user_input = input(
            "Download JanusX Singularity Image (~800MB)? [y/N or /path/to.sif]: "
        ).strip()
        if not user_input or user_input.lower() == "n":
            raise SystemExit(1)
        tmp_path = binary_path / "janusxext.tmp.sif"
        if user_input.lower() == "y":
            downloader(
                "https://github.com/FJingxian/JanusX/releases/download/v1.0.10/janusxext.sif",
                tmp_path,
            )
        else:
            src = Path(user_input).expanduser().resolve()
            if not src.exists():
                raise SystemExit(f"SIF path not found: {src}")
            shutil.copy2(src, tmp_path)

        subprocess.run([str(tmp_path), "gatk", "-version"], shell=True, check=True)
        os.replace(tmp_path, sif_path)

    singularity_prefix = f"singularity exec {sif_path} "
    logger.info(f"Singularity: {singularity_prefix}")

    cmd = indexREF(str(reference), singularity=singularity_prefix)
    logger.info(f"Indexing reference: {cmd}")
    index_job = wrap_cmd(cmd, "indexREF", 1, scheduler=args.backend)
    subprocess.run(index_job, shell=True, check=True, cwd=workdir)
    fai_path = Path(f"{reference}.fai")
    if not fai_path.exists():
        logger.info("Waiting for FASTA index (.fai) to be ready...")
        while not fai_path.exists():
            time.sleep(5)

    chrom = read_chroms_from_fai(reference)
    logger.info(f"Chromosomes: {len(chrom)}")

    fastqlist = sorted(
        p for p in fastq_dir.rglob("*")
        if p.is_file() and p.name.lower().endswith(FASTQ_SUFFIXES)
    )
    if not fastqlist:
        raise FileNotFoundError(f"No FASTQ files found in {fastq_dir}")

    level1, level2 = two_level_cluster(fastqlist)
    samples = build_samples(level1, level2)
    logger.info(f"Samples: {len(samples)}")

    metadata = {
        "reference": str(reference),
        "samples": {k: [str(v[0]), str(v[1])] for k, v in samples.items()},
        "chrom": chrom,
    }

    os.chdir(workdir)
    fastq2vcf(
        metadata=metadata,
        workdir=workdir,
        backbend=args.backend,
        nohup_max_jobs=args.maxtask,
        singularity=singularity_prefix,
    )

    logger.info(f"Pipeline finished in {time.time() - t_start:.1f}s")


if __name__ == "__main__":
    main()
